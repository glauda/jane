{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# init NLP objects\n",
    "stopwords = nltk.corpus.stopwords.words('english') #+ nltk.corpus.stopwords.words('french')\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.stem.porter.PorterStemmer"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(porter_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "\n",
    "#defining the function to remove punctuation\n",
    "def remove_punctuation(text: str):\n",
    "    punctuation_free = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuation_free\n",
    "\n",
    "def tokenization(text: str) -> List[str]:\n",
    "    # tokens = re.split('W+',text)\n",
    "    l_tokens = re.findall(r'(?i)((?:[a-z]|\\')+)', text)\n",
    "    return l_tokens\n",
    "\n",
    "def remove_stopwords(l_tokens: List[str]) -> List[str]:\n",
    "    l_output = [token for token in l_tokens if token not in stopwords]\n",
    "    return l_output\n",
    "\n",
    "def stemming(l_tokens: List[str]) -> List[str]:\n",
    "    l_stem_text = [porter_stemmer.stem(word) for word in l_tokens]\n",
    "    return l_stem_text\n",
    "\n",
    "def lemmatizer(l_tokens: List[str]) -> List[str]:\n",
    "    l_lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in l_tokens]\n",
    "    return l_lemm_text\n",
    "\n",
    "def filter_len(l_tokens: List[str], limit: int) -> List[str]:\n",
    "    l_filtered = [token for token in l_tokens if len(token) > limit]\n",
    "    return l_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/01_raw/medium_extract.parquet\")\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>day</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why decorators in Python are pure genius | by ...</td>\n",
       "      <td>Ari Joury</td>\n",
       "      <td>https://towardsdatascience.com/why-decorators-...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nIf th...</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Ultimate Guide to Cracking Product Case In...</td>\n",
       "      <td>Emma Ding</td>\n",
       "      <td>https://towardsdatascience.com/the-ultimate-gu...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nWritt...</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit() vs predict() vs fit_predict() in Python ...</td>\n",
       "      <td>Giorgos Myrianthous</td>\n",
       "      <td>https://towardsdatascience.com/fit-vs-predict-...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nsciki...</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stepwise Regression Tutorial in Python | by Ry...</td>\n",
       "      <td>Ryan Kwok</td>\n",
       "      <td>https://towardsdatascience.com/stepwise-regres...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nHow d...</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7 Must-Know Data Wrangling Operations with Pyt...</td>\n",
       "      <td>Soner Yıldırım</td>\n",
       "      <td>https://towardsdatascience.com/7-must-know-dat...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nA com...</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>Expanding Review Inequality in Madden | by Cod...</td>\n",
       "      <td>Cody Glickman, PhD</td>\n",
       "      <td>https://towardsdatascience.com/expanding-revie...</td>\n",
       "      <td>Towards Data Science\\nNov 23, 2021\\nSave\\nIn 2...</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>1769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>Geocoding Locations with Turf Mapbox SDK | by ...</td>\n",
       "      <td>Charmaine Chui</td>\n",
       "      <td>https://towardsdatascience.com/geocoding-locat...</td>\n",
       "      <td>Towards Data Science\\nNov 23, 2021\\nSave\\nMost...</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>1770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>A No-Code Method of Mapping UFO Sightings with...</td>\n",
       "      <td>Shreya Chaudhary</td>\n",
       "      <td>https://towardsdatascience.com/a-no-code-metho...</td>\n",
       "      <td>Towards Data Science\\nNov 23, 2021\\nSave\\nIn t...</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>1771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>Fines Migrate in Espresso, but Not Far: Part 3...</td>\n",
       "      <td>Robert McKeon Aloe</td>\n",
       "      <td>https://towardsdatascience.com/fines-migrate-i...</td>\n",
       "      <td>Towards Data Science\\nNov 23, 2021\\nSave\\nOne ...</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>1772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>How To Clean A Dataset By Following These 4 Gu...</td>\n",
       "      <td>Susan Maina</td>\n",
       "      <td>https://towardsdatascience.com/the-ultimate-4-...</td>\n",
       "      <td>Towards Data Science\\nMay 28, 2021\\nSave\\nData...</td>\n",
       "      <td>2021-05-28</td>\n",
       "      <td>1799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1722 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title               author  \\\n",
       "0     Why decorators in Python are pure genius | by ...            Ari Joury   \n",
       "1     The Ultimate Guide to Cracking Product Case In...            Emma Ding   \n",
       "2     fit() vs predict() vs fit_predict() in Python ...  Giorgos Myrianthous   \n",
       "3     Stepwise Regression Tutorial in Python | by Ry...            Ryan Kwok   \n",
       "4     7 Must-Know Data Wrangling Operations with Pyt...       Soner Yıldırım   \n",
       "...                                                 ...                  ...   \n",
       "1769  Expanding Review Inequality in Madden | by Cod...   Cody Glickman, PhD   \n",
       "1770  Geocoding Locations with Turf Mapbox SDK | by ...       Charmaine Chui   \n",
       "1771  A No-Code Method of Mapping UFO Sightings with...     Shreya Chaudhary   \n",
       "1772  Fines Migrate in Espresso, but Not Far: Part 3...   Robert McKeon Aloe   \n",
       "1799  How To Clean A Dataset By Following These 4 Gu...          Susan Maina   \n",
       "\n",
       "                                                   link  \\\n",
       "0     https://towardsdatascience.com/why-decorators-...   \n",
       "1     https://towardsdatascience.com/the-ultimate-gu...   \n",
       "2     https://towardsdatascience.com/fit-vs-predict-...   \n",
       "3     https://towardsdatascience.com/stepwise-regres...   \n",
       "4     https://towardsdatascience.com/7-must-know-dat...   \n",
       "...                                                 ...   \n",
       "1769  https://towardsdatascience.com/expanding-revie...   \n",
       "1770  https://towardsdatascience.com/geocoding-locat...   \n",
       "1771  https://towardsdatascience.com/a-no-code-metho...   \n",
       "1772  https://towardsdatascience.com/fines-migrate-i...   \n",
       "1799  https://towardsdatascience.com/the-ultimate-4-...   \n",
       "\n",
       "                                                   text         day  index  \n",
       "0     Towards Data Science\\nMar 9, 2021\\nSave\\nIf th...  2021-03-09      0  \n",
       "1     Towards Data Science\\nMar 9, 2021\\nSave\\nWritt...  2021-03-09      1  \n",
       "2     Towards Data Science\\nMar 9, 2021\\nSave\\nsciki...  2021-03-09      2  \n",
       "3     Towards Data Science\\nMar 9, 2021\\nSave\\nHow d...  2021-03-09      3  \n",
       "4     Towards Data Science\\nMar 9, 2021\\nSave\\nA com...  2021-03-09      4  \n",
       "...                                                 ...         ...    ...  \n",
       "1769  Towards Data Science\\nNov 23, 2021\\nSave\\nIn 2...  2021-11-23   1769  \n",
       "1770  Towards Data Science\\nNov 23, 2021\\nSave\\nMost...  2021-11-23   1770  \n",
       "1771  Towards Data Science\\nNov 23, 2021\\nSave\\nIn t...  2021-11-23   1771  \n",
       "1772  Towards Data Science\\nNov 23, 2021\\nSave\\nOne ...  2021-11-23   1772  \n",
       "1799  Towards Data Science\\nMay 28, 2021\\nSave\\nData...  2021-05-28   1799  \n",
       "\n",
       "[1722 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.day.value_counts()\n",
    "# df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"index\"] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "df = df.assign(text = df.text.str.lower())\n",
    "df = df.assign(text = df.text.apply(lambda x: tokenization(x)))\n",
    "df = df.assign(text = df.text.apply(lambda x: remove_stopwords(x)))\n",
    "df = df.assign(text = df.text.apply(lambda x: stemming(x)))\n",
    "df = df.assign(text = df.text.apply(lambda x: lemmatizer(x)))\n",
    "df = df.assign(text = df.text.apply(lambda x: filter_len(x, 2)))\n",
    "\n",
    "# Adapt format for sklearn's TF-IDF\n",
    "df = df.assign(text = df.text.apply(lambda x: \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    toward data scienc mar save one thing make pyt...\n",
       "1    toward data scienc mar save written emma ding ...\n",
       "2    toward data scienc mar save scikit learn commo...\n",
       "3    toward data scienc mar save find mean data min...\n",
       "4    toward data scienc mar save comprehens practic...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "TF_IDF = vectorizer.fit(df_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_enc = pd.DataFrame(TF_IDF.transform(df_test.text).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31097</th>\n",
       "      <th>31098</th>\n",
       "      <th>31099</th>\n",
       "      <th>31100</th>\n",
       "      <th>31101</th>\n",
       "      <th>31102</th>\n",
       "      <th>31103</th>\n",
       "      <th>31104</th>\n",
       "      <th>31105</th>\n",
       "      <th>31106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 31107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1      2      3      4      5      6      7      8      9      \\\n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "340    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "341    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "342    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "343    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "344    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "     ...  31097  31098  31099  31100  31101  31102  31103  31104  31105  31106  \n",
       "0    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "340  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "341  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "342  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "343  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "344  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[345 rows x 31107 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TF_IDF.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax_labels = TF_IDF.get_feature_names_out()\n",
    "n_labels = ax_labels.shape[0]\n",
    "\n",
    "d_labels = {i: ax_labels[i] for i in range(n_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_enc = df_enc.rename(columns=d_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaai</th>\n",
       "      <th>aab</th>\n",
       "      <th>aac</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aad</th>\n",
       "      <th>aadidev</th>\n",
       "      <th>aaf</th>\n",
       "      <th>aafdb</th>\n",
       "      <th>aagesen</th>\n",
       "      <th>...</th>\n",
       "      <th>zwiebel</th>\n",
       "      <th>zwift</th>\n",
       "      <th>zwikirsch</th>\n",
       "      <th>zxcvbnm</th>\n",
       "      <th>zxh</th>\n",
       "      <th>zykov</th>\n",
       "      <th>zyl</th>\n",
       "      <th>zyrobot</th>\n",
       "      <th>zyte</th>\n",
       "      <th>zzhu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 31107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aaa  aaai  aab  aac  aachen  aad  aadidev  aaf  aafdb  aagesen  ...  \\\n",
       "0    0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "1    0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "2    0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "3    0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "4    0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "..   ...   ...  ...  ...     ...  ...      ...  ...    ...      ...  ...   \n",
       "340  0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "341  0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "342  0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "343  0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "344  0.0   0.0  0.0  0.0     0.0  0.0      0.0  0.0    0.0      0.0  ...   \n",
       "\n",
       "     zwiebel  zwift  zwikirsch  zxcvbnm  zxh  zykov  zyl  zyrobot  zyte  zzhu  \n",
       "0        0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "1        0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "2        0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "3        0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "4        0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "..       ...    ...        ...      ...  ...    ...  ...      ...   ...   ...  \n",
       "340      0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "341      0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "342      0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "343      0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "344      0.0    0.0        0.0      0.0  0.0    0.0  0.0      0.0   0.0   0.0  \n",
       "\n",
       "[345 rows x 31107 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dashboard', 'chart', 'dash', 'graph', 'style']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = 2\n",
    "\n",
    "s_doc = df_enc.loc[doc_id, :]\n",
    "s_doc.sort_values(ascending=False)[:5].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>I Forgot How to Spellcheck | by Victor Shepele...</td>\n",
       "      <td>Victor Shepelev</td>\n",
       "      <td>https://towardsdatascience.com/i-forgot-how-to...</td>\n",
       "      <td>toward data scienc may save start rebuild worl...</td>\n",
       "      <td>2021-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>How To Delete A Column In Pandas | Towards Dat...</td>\n",
       "      <td>Giorgos Myrianthous</td>\n",
       "      <td>https://towardsdatascience.com/how-to-delete-a...</td>\n",
       "      <td>toward data scienc aug save delet column panda...</td>\n",
       "      <td>2021-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Serie A (football) — a simple dashboard with P...</td>\n",
       "      <td>Xiao Wang</td>\n",
       "      <td>https://towardsdatascience.com/create-a-simple...</td>\n",
       "      <td>toward data scienc mar save sinc last time pla...</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>Anonymise Sensitive Data in a Pandas DataFrame...</td>\n",
       "      <td>Rhys Kilian</td>\n",
       "      <td>https://towardsdatascience.com/anonymise-sensi...</td>\n",
       "      <td>toward data scienc sep save common scenario en...</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>The paradox in protecting personal information...</td>\n",
       "      <td>Simone Jeurissen</td>\n",
       "      <td>https://towardsdatascience.com/the-paradox-in-...</td>\n",
       "      <td>toward data scienc mar save walk street maastr...</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>A Simple Guide to Linear Regression using Pyth...</td>\n",
       "      <td>Frank Andrade</td>\n",
       "      <td>https://towardsdatascience.com/a-simple-guide-...</td>\n",
       "      <td>toward data scienc oct save one first machin l...</td>\n",
       "      <td>2021-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>Visual Studio Code is Now Available as a Web A...</td>\n",
       "      <td>Dario Radečić</td>\n",
       "      <td>https://towardsdatascience.com/visual-studio-c...</td>\n",
       "      <td>toward data scienc nov save visual studio code...</td>\n",
       "      <td>2021-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Secure Password Handling in Python | by Martin...</td>\n",
       "      <td>Martin Heinz</td>\n",
       "      <td>https://towardsdatascience.com/secure-password...</td>\n",
       "      <td>toward data scienc oct save almost everi appli...</td>\n",
       "      <td>2021-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>What is a Data Hub?. Why Architectures such as...</td>\n",
       "      <td>Christianlauer</td>\n",
       "      <td>https://towardsdatascience.com/what-is-a-data-...</td>\n",
       "      <td>toward data scienc jan save data hub data exch...</td>\n",
       "      <td>2021-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>Beyond Codex: A Code Generation Model That You...</td>\n",
       "      <td>Amine Elhattami</td>\n",
       "      <td>https://towardsdatascience.com/beyond-codex-a-...</td>\n",
       "      <td>toward data scienc nov save recent releas open...</td>\n",
       "      <td>2021-11-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title               author  \\\n",
       "300   I Forgot How to Spellcheck | by Victor Shepele...      Victor Shepelev   \n",
       "1075  How To Delete A Column In Pandas | Towards Dat...  Giorgos Myrianthous   \n",
       "535   Serie A (football) — a simple dashboard with P...            Xiao Wang   \n",
       "1318  Anonymise Sensitive Data in a Pandas DataFrame...          Rhys Kilian   \n",
       "561   The paradox in protecting personal information...     Simone Jeurissen   \n",
       "...                                                 ...                  ...   \n",
       "101   A Simple Guide to Linear Regression using Pyth...        Frank Andrade   \n",
       "1714  Visual Studio Code is Now Available as a Web A...      Dario Radečić   \n",
       "107   Secure Password Handling in Python | by Martin...         Martin Heinz   \n",
       "1656  What is a Data Hub?. Why Architectures such as...       Christianlauer   \n",
       "1755  Beyond Codex: A Code Generation Model That You...      Amine Elhattami   \n",
       "\n",
       "                                                   link  \\\n",
       "300   https://towardsdatascience.com/i-forgot-how-to...   \n",
       "1075  https://towardsdatascience.com/how-to-delete-a...   \n",
       "535   https://towardsdatascience.com/create-a-simple...   \n",
       "1318  https://towardsdatascience.com/anonymise-sensi...   \n",
       "561   https://towardsdatascience.com/the-paradox-in-...   \n",
       "...                                                 ...   \n",
       "101   https://towardsdatascience.com/a-simple-guide-...   \n",
       "1714  https://towardsdatascience.com/visual-studio-c...   \n",
       "107   https://towardsdatascience.com/secure-password...   \n",
       "1656  https://towardsdatascience.com/what-is-a-data-...   \n",
       "1755  https://towardsdatascience.com/beyond-codex-a-...   \n",
       "\n",
       "                                                   text         day  \n",
       "300   toward data scienc may save start rebuild worl...  2021-05-06  \n",
       "1075  toward data scienc aug save delet column panda...  2021-08-11  \n",
       "535   toward data scienc mar save sinc last time pla...  2021-03-01  \n",
       "1318  toward data scienc sep save common scenario en...  2021-09-01  \n",
       "561   toward data scienc mar save walk street maastr...  2021-03-01  \n",
       "...                                                 ...         ...  \n",
       "101   toward data scienc oct save one first machin l...  2021-10-18  \n",
       "1714  toward data scienc nov save visual studio code...  2021-11-04  \n",
       "107   toward data scienc oct save almost everi appli...  2021-10-18  \n",
       "1656  toward data scienc jan save data hub data exch...  2021-01-18  \n",
       "1755  toward data scienc nov save recent releas open...  2021-11-23  \n",
       "\n",
       "[345 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(\"../data/03_primary/articles.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why decorators in Python are pure genius | by ...</td>\n",
       "      <td>Ari Joury</td>\n",
       "      <td>https://towardsdatascience.com/why-decorators-...</td>\n",
       "      <td>toward data scienc mar save one thing make pyt...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Ultimate Guide to Cracking Product Case In...</td>\n",
       "      <td>Emma Ding</td>\n",
       "      <td>https://towardsdatascience.com/the-ultimate-gu...</td>\n",
       "      <td>toward data scienc mar save written emma ding ...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit() vs predict() vs fit_predict() in Python ...</td>\n",
       "      <td>Giorgos Myrianthous</td>\n",
       "      <td>https://towardsdatascience.com/fit-vs-predict-...</td>\n",
       "      <td>toward data scienc mar save scikit learn commo...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stepwise Regression Tutorial in Python | by Ry...</td>\n",
       "      <td>Ryan Kwok</td>\n",
       "      <td>https://towardsdatascience.com/stepwise-regres...</td>\n",
       "      <td>toward data scienc mar save find mean data min...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7 Must-Know Data Wrangling Operations with Pyt...</td>\n",
       "      <td>Soner Yıldırım</td>\n",
       "      <td>https://towardsdatascience.com/7-must-know-dat...</td>\n",
       "      <td>toward data scienc mar save comprehens practic...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>Climate change: Interactive Plotly Dash to dis...</td>\n",
       "      <td>Francesco Tontarelli</td>\n",
       "      <td>https://towardsdatascience.com/climate-change-...</td>\n",
       "      <td>toward data scienc may save author camilla mas...</td>\n",
       "      <td>2021-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>Where eBay Went Right — and Wrong — with AI: W...</td>\n",
       "      <td>Wilson Pang</td>\n",
       "      <td>https://towardsdatascience.com/where-ebay-went...</td>\n",
       "      <td>toward data scienc may save follow adapt real ...</td>\n",
       "      <td>2021-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>Automatically track all your EDA using Sweetvi...</td>\n",
       "      <td>Francois Bertrand</td>\n",
       "      <td>https://towardsdatascience.com/automatically-t...</td>\n",
       "      <td>toward data scienc may save sweetviz great lib...</td>\n",
       "      <td>2021-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>The White Rhinos of Linguistics | by Yaning Wu...</td>\n",
       "      <td>Yaning Wu</td>\n",
       "      <td>https://towardsdatascience.com/the-white-rhino...</td>\n",
       "      <td>toward data scienc may save piec best experien...</td>\n",
       "      <td>2021-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>Interpretation, interpolation and other lies |...</td>\n",
       "      <td>Corné Potgieter</td>\n",
       "      <td>https://towardsdatascience.com/interpretation-...</td>\n",
       "      <td>toward data scienc may save know year world re...</td>\n",
       "      <td>2021-05-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1817 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title                author  \\\n",
       "0     Why decorators in Python are pure genius | by ...             Ari Joury   \n",
       "1     The Ultimate Guide to Cracking Product Case In...             Emma Ding   \n",
       "2     fit() vs predict() vs fit_predict() in Python ...   Giorgos Myrianthous   \n",
       "3     Stepwise Regression Tutorial in Python | by Ry...             Ryan Kwok   \n",
       "4     7 Must-Know Data Wrangling Operations with Pyt...        Soner Yıldırım   \n",
       "...                                                 ...                   ...   \n",
       "1812  Climate change: Interactive Plotly Dash to dis...  Francesco Tontarelli   \n",
       "1813  Where eBay Went Right — and Wrong — with AI: W...           Wilson Pang   \n",
       "1814  Automatically track all your EDA using Sweetvi...     Francois Bertrand   \n",
       "1815  The White Rhinos of Linguistics | by Yaning Wu...             Yaning Wu   \n",
       "1816  Interpretation, interpolation and other lies |...      Corné Potgieter   \n",
       "\n",
       "                                                   link  \\\n",
       "0     https://towardsdatascience.com/why-decorators-...   \n",
       "1     https://towardsdatascience.com/the-ultimate-gu...   \n",
       "2     https://towardsdatascience.com/fit-vs-predict-...   \n",
       "3     https://towardsdatascience.com/stepwise-regres...   \n",
       "4     https://towardsdatascience.com/7-must-know-dat...   \n",
       "...                                                 ...   \n",
       "1812  https://towardsdatascience.com/climate-change-...   \n",
       "1813  https://towardsdatascience.com/where-ebay-went...   \n",
       "1814  https://towardsdatascience.com/automatically-t...   \n",
       "1815  https://towardsdatascience.com/the-white-rhino...   \n",
       "1816  https://towardsdatascience.com/interpretation-...   \n",
       "\n",
       "                                                   text         day  \n",
       "0     toward data scienc mar save one thing make pyt...  2021-03-09  \n",
       "1     toward data scienc mar save written emma ding ...  2021-03-09  \n",
       "2     toward data scienc mar save scikit learn commo...  2021-03-09  \n",
       "3     toward data scienc mar save find mean data min...  2021-03-09  \n",
       "4     toward data scienc mar save comprehens practic...  2021-03-09  \n",
       "...                                                 ...         ...  \n",
       "1812  toward data scienc may save author camilla mas...  2021-05-28  \n",
       "1813  toward data scienc may save follow adapt real ...  2021-05-28  \n",
       "1814  toward data scienc may save sweetviz great lib...  2021-05-28  \n",
       "1815  toward data scienc may save piec best experien...  2021-05-28  \n",
       "1816  toward data scienc may save know year world re...  2021-05-28  \n",
       "\n",
       "[1817 rows x 5 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TF_IDF)\n",
    "type(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "tf_idf = joblib.load(\"../data/06_models/tf_idf.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why decorators in Python are pure genius | by ...</td>\n",
       "      <td>Ari Joury</td>\n",
       "      <td>https://towardsdatascience.com/why-decorators-...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nIf th...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Ultimate Guide to Cracking Product Case In...</td>\n",
       "      <td>Emma Ding</td>\n",
       "      <td>https://towardsdatascience.com/the-ultimate-gu...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nWritt...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit() vs predict() vs fit_predict() in Python ...</td>\n",
       "      <td>Giorgos Myrianthous</td>\n",
       "      <td>https://towardsdatascience.com/fit-vs-predict-...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nsciki...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stepwise Regression Tutorial in Python | by Ry...</td>\n",
       "      <td>Ryan Kwok</td>\n",
       "      <td>https://towardsdatascience.com/stepwise-regres...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nHow d...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7 Must-Know Data Wrangling Operations with Pyt...</td>\n",
       "      <td>Soner Yıldırım</td>\n",
       "      <td>https://towardsdatascience.com/7-must-know-dat...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nA com...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>Expanding Review Inequality in Madden | by Cod...</td>\n",
       "      <td>Cody Glickman, PhD</td>\n",
       "      <td>https://towardsdatascience.com/expanding-revie...</td>\n",
       "      <td>Towards Data Science\\nNov 23, 2021\\nSave\\nIn 2...</td>\n",
       "      <td>2021-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>Geocoding Locations with Turf Mapbox SDK | by ...</td>\n",
       "      <td>Charmaine Chui</td>\n",
       "      <td>https://towardsdatascience.com/geocoding-locat...</td>\n",
       "      <td>Towards Data Science\\nNov 23, 2021\\nSave\\nMost...</td>\n",
       "      <td>2021-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>A No-Code Method of Mapping UFO Sightings with...</td>\n",
       "      <td>Shreya Chaudhary</td>\n",
       "      <td>https://towardsdatascience.com/a-no-code-metho...</td>\n",
       "      <td>Towards Data Science\\nNov 23, 2021\\nSave\\nIn t...</td>\n",
       "      <td>2021-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>Fines Migrate in Espresso, but Not Far: Part 3...</td>\n",
       "      <td>Robert McKeon Aloe</td>\n",
       "      <td>https://towardsdatascience.com/fines-migrate-i...</td>\n",
       "      <td>Towards Data Science\\nNov 23, 2021\\nSave\\nOne ...</td>\n",
       "      <td>2021-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>How To Clean A Dataset By Following These 4 Gu...</td>\n",
       "      <td>Susan Maina</td>\n",
       "      <td>https://towardsdatascience.com/the-ultimate-4-...</td>\n",
       "      <td>Towards Data Science\\nMay 28, 2021\\nSave\\nData...</td>\n",
       "      <td>2021-05-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1722 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title               author  \\\n",
       "0     Why decorators in Python are pure genius | by ...            Ari Joury   \n",
       "1     The Ultimate Guide to Cracking Product Case In...            Emma Ding   \n",
       "2     fit() vs predict() vs fit_predict() in Python ...  Giorgos Myrianthous   \n",
       "3     Stepwise Regression Tutorial in Python | by Ry...            Ryan Kwok   \n",
       "4     7 Must-Know Data Wrangling Operations with Pyt...       Soner Yıldırım   \n",
       "...                                                 ...                  ...   \n",
       "1769  Expanding Review Inequality in Madden | by Cod...   Cody Glickman, PhD   \n",
       "1770  Geocoding Locations with Turf Mapbox SDK | by ...       Charmaine Chui   \n",
       "1771  A No-Code Method of Mapping UFO Sightings with...     Shreya Chaudhary   \n",
       "1772  Fines Migrate in Espresso, but Not Far: Part 3...   Robert McKeon Aloe   \n",
       "1799  How To Clean A Dataset By Following These 4 Gu...          Susan Maina   \n",
       "\n",
       "                                                   link  \\\n",
       "0     https://towardsdatascience.com/why-decorators-...   \n",
       "1     https://towardsdatascience.com/the-ultimate-gu...   \n",
       "2     https://towardsdatascience.com/fit-vs-predict-...   \n",
       "3     https://towardsdatascience.com/stepwise-regres...   \n",
       "4     https://towardsdatascience.com/7-must-know-dat...   \n",
       "...                                                 ...   \n",
       "1769  https://towardsdatascience.com/expanding-revie...   \n",
       "1770  https://towardsdatascience.com/geocoding-locat...   \n",
       "1771  https://towardsdatascience.com/a-no-code-metho...   \n",
       "1772  https://towardsdatascience.com/fines-migrate-i...   \n",
       "1799  https://towardsdatascience.com/the-ultimate-4-...   \n",
       "\n",
       "                                                   text         day  \n",
       "0     Towards Data Science\\nMar 9, 2021\\nSave\\nIf th...  2021-03-09  \n",
       "1     Towards Data Science\\nMar 9, 2021\\nSave\\nWritt...  2021-03-09  \n",
       "2     Towards Data Science\\nMar 9, 2021\\nSave\\nsciki...  2021-03-09  \n",
       "3     Towards Data Science\\nMar 9, 2021\\nSave\\nHow d...  2021-03-09  \n",
       "4     Towards Data Science\\nMar 9, 2021\\nSave\\nA com...  2021-03-09  \n",
       "...                                                 ...         ...  \n",
       "1769  Towards Data Science\\nNov 23, 2021\\nSave\\nIn 2...  2021-11-23  \n",
       "1770  Towards Data Science\\nNov 23, 2021\\nSave\\nMost...  2021-11-23  \n",
       "1771  Towards Data Science\\nNov 23, 2021\\nSave\\nIn t...  2021-11-23  \n",
       "1772  Towards Data Science\\nNov 23, 2021\\nSave\\nOne ...  2021-11-23  \n",
       "1799  Towards Data Science\\nMay 28, 2021\\nSave\\nData...  2021-05-28  \n",
       "\n",
       "[1722 rows x 5 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why decorators in Python are pure genius | by ...</td>\n",
       "      <td>Ari Joury</td>\n",
       "      <td>https://towardsdatascience.com/why-decorators-...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nIf th...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Ultimate Guide to Cracking Product Case In...</td>\n",
       "      <td>Emma Ding</td>\n",
       "      <td>https://towardsdatascience.com/the-ultimate-gu...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nWritt...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit() vs predict() vs fit_predict() in Python ...</td>\n",
       "      <td>Giorgos Myrianthous</td>\n",
       "      <td>https://towardsdatascience.com/fit-vs-predict-...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nsciki...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stepwise Regression Tutorial in Python | by Ry...</td>\n",
       "      <td>Ryan Kwok</td>\n",
       "      <td>https://towardsdatascience.com/stepwise-regres...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nHow d...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7 Must-Know Data Wrangling Operations with Pyt...</td>\n",
       "      <td>Soner Yıldırım</td>\n",
       "      <td>https://towardsdatascience.com/7-must-know-dat...</td>\n",
       "      <td>Towards Data Science\\nMar 9, 2021\\nSave\\nA com...</td>\n",
       "      <td>2021-03-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title               author  \\\n",
       "0  Why decorators in Python are pure genius | by ...            Ari Joury   \n",
       "1  The Ultimate Guide to Cracking Product Case In...            Emma Ding   \n",
       "2  fit() vs predict() vs fit_predict() in Python ...  Giorgos Myrianthous   \n",
       "3  Stepwise Regression Tutorial in Python | by Ry...            Ryan Kwok   \n",
       "4  7 Must-Know Data Wrangling Operations with Pyt...       Soner Yıldırım   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://towardsdatascience.com/why-decorators-...   \n",
       "1  https://towardsdatascience.com/the-ultimate-gu...   \n",
       "2  https://towardsdatascience.com/fit-vs-predict-...   \n",
       "3  https://towardsdatascience.com/stepwise-regres...   \n",
       "4  https://towardsdatascience.com/7-must-know-dat...   \n",
       "\n",
       "                                                text         day  \n",
       "0  Towards Data Science\\nMar 9, 2021\\nSave\\nIf th...  2021-03-09  \n",
       "1  Towards Data Science\\nMar 9, 2021\\nSave\\nWritt...  2021-03-09  \n",
       "2  Towards Data Science\\nMar 9, 2021\\nSave\\nsciki...  2021-03-09  \n",
       "3  Towards Data Science\\nMar 9, 2021\\nSave\\nHow d...  2021-03-09  \n",
       "4  Towards Data Science\\nMar 9, 2021\\nSave\\nA com...  2021-03-09  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(\"../data/05_model_input/test.parquet\")\n",
    "tf_idf = joblib.load(\"../data/06_models/tf_idf.pkl\")\n",
    "\n",
    "df_test[\"index\"] = df_test.index\n",
    "df_test = df_test.rename(columns={\"index\": \"id\"})\n",
    "df_enc = pd.DataFrame(tf_idf.transform(df_test.text).toarray())\n",
    "\n",
    "# Create column id / label mapping\n",
    "ax_labels = tf_idf.get_feature_names_out()\n",
    "n_labels = ax_labels.shape[0]\n",
    "d_labels = {i: ax_labels[i] for i in range(n_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31277</th>\n",
       "      <th>31278</th>\n",
       "      <th>31279</th>\n",
       "      <th>31280</th>\n",
       "      <th>31281</th>\n",
       "      <th>31282</th>\n",
       "      <th>31283</th>\n",
       "      <th>31284</th>\n",
       "      <th>31285</th>\n",
       "      <th>31286</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364 rows × 31287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1      2      3      4      5      6      7      8      9      \\\n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "359    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "360    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "361    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "362    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "363    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "     ...  31277  31278  31279  31280  31281  31282  31283  31284  31285  31286  \n",
       "0    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "359  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "360  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "361  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "362  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "363  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[364 rows x 31287 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_id = 2\n",
    "n_topics = 5\n",
    "\n",
    "d_topics = {}\n",
    "\n",
    "for index, row in df_enc.iterrows():\n",
    "    # print(row)\n",
    "    # print(index)\n",
    "    # article_id = df_test.loc[index, \"id\"]\n",
    "    # s_doc = df_enc.loc[doc_id, :]\n",
    "    article_id = df_test.loc[index, \"id\"]\n",
    "    l_topics = row.sort_values(ascending=False)[:n_topics].index.tolist()\n",
    "    l_topics = [d_labels[elt] for elt in l_topics]\n",
    "\n",
    "    d_topics[article_id] = l_topics\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1507: ['arm', 'sampl', 'bandit', 'regret', 'algorithm']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "d_res = joblib.load(\"../data/07_model_output/test_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = df_test = pd.read_parquet(\"../data/05_model_input/train.parquet\")\n",
    "df_test = pd.read_parquet(\"../data/05_model_input/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>526</td>\n",
       "      <td>Feature Selection, Binning, ANOVA, polynomial features, log transform, automatic feature selection | Towards Data Science</td>\n",
       "      <td>Ibrahim Kovan</td>\n",
       "      <td>https://towardsdatascience.com/an-overview-of-data-preprocessing-features-enrichment-automatic-feature-selection-60b0c12d75ad?source=collection_archive---------14-----------------------</td>\n",
       "      <td>toward data scienc aug save dataset render suitabl data train machin learn predict made algorithm yield success result look dataset seen featur import other impact output exampl better result obta...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>354</td>\n",
       "      <td>Top Python libraries for Image Augmentation in Computer Vision | by Kenneth Leung | Towards Data Science</td>\n",
       "      <td>Kenneth Leung</td>\n",
       "      <td>https://towardsdatascience.com/top-python-libraries-for-image-augmentation-in-computer-vision-2566bed0533e?source=collection_archive---------2-----------------------</td>\n",
       "      <td>toward data scienc aug save deep learn task like comput vision highli depend larg number imag train techniqu like transfer learn reduc amount data need dataset suffici qualiti quantiti varieti sti...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>168</td>\n",
       "      <td>Image Classification of PCBs and its Web Application (Flask) | by Utkarsh Ankit | Towards Data Science</td>\n",
       "      <td>Utkarsh Ankit</td>\n",
       "      <td>https://towardsdatascience.com/image-classification-of-pcbs-and-its-web-application-flask-c2b26039924a?source=collection_archive---------8-----------------------</td>\n",
       "      <td>toward data scienc aug save hello blog creat imag classif model pcb print circuit board detect defect pcb classifi good bad creat deep learn model tri get best possibl result along proper visualis...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>Variational Bayesian inference with normalizing flows: a simple example | by Fraser Lewis | Towards Data Science</td>\n",
       "      <td>Fraser Lewis</td>\n",
       "      <td>https://towardsdatascience.com/variational-bayesian-inference-with-normalizing-flows-a-simple-example-1db109d91062?source=collection_archive---------7-----------------------</td>\n",
       "      <td>toward data scienc aug save variat infer methodolog watch larg complex bayesian model potenti becom approach least comput fit natur domain machin learn articl demonstr normal flow briefli introduc...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>937</td>\n",
       "      <td>What does Vertex AI do?. Your one-stop shop for machine learning | by Omer Mahmood | Towards Data Science</td>\n",
       "      <td>Omer Mahmood</td>\n",
       "      <td>https://towardsdatascience.com/what-does-vertex-ai-do-d30014024f55?source=collection_archive---------9-----------------------</td>\n",
       "      <td>toward data scienc aug save post cover common task typic machin learn workflow vertex bring togeth tool need achiev one unifi user interfac today data scientist grappl challeng manual piec togeth ...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>1074</td>\n",
       "      <td>Machine Learning with only SQL — Using BigQuery to Identify the Target Audience for Shared Bike | by Wan Chung Huang | Towards Data Science</td>\n",
       "      <td>Wan Chung Huang</td>\n",
       "      <td>https://towardsdatascience.com/machine-learning-with-only-sql-using-bigquery-to-identify-the-target-audience-for-shared-bike-aa3a4041be3a?source=collection_archive---------18-----------------------</td>\n",
       "      <td>toward data scienc aug save mani reason use python machin learn matter case share machin learn sql understand key featur influenc durat bike share trip observ develop data warehous averag durat bi...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>1315</td>\n",
       "      <td>Build a Dash app with Python in 7 minutes | by Natassha Selvaraj | Towards Data Science</td>\n",
       "      <td>Natassha Selvaraj</td>\n",
       "      <td>https://towardsdatascience.com/build-a-dash-app-with-python-in-7-minutes-72b6cca7d268?source=collection_archive---------3-----------------------</td>\n",
       "      <td>toward data scienc aug save start work capston project month ago want creat interact dashboard machin learn model want dashboard form web applic display live updat new data enter system explor man...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>383</td>\n",
       "      <td>Styleformer: Convert Casual Text to Formal Text and Vice Versa | by Eric Fillion | Towards Data Science</td>\n",
       "      <td>Eric Fillion</td>\n",
       "      <td>https://towardsdatascience.com/styleformer-convert-casual-text-to-formal-text-and-vice-versa-9cdc52abeaf5?source=collection_archive---------31-----------------------</td>\n",
       "      <td>toward data scienc aug save styleform brand new python librari allow chang style text use power transform model call tutori focus abil convert casual text formal text vice versa chang formal text ...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>1273</td>\n",
       "      <td>Ensemble Classification: A Brief Overview With Examples | by Pranav Thaenraj | Towards Data Science</td>\n",
       "      <td>Pranav Thaenraj</td>\n",
       "      <td>https://towardsdatascience.com/ensemble-classification-a-brief-overview-with-examples-3dac25613073?source=collection_archive---------25-----------------------</td>\n",
       "      <td>toward data scienc aug save note articl final articl seri articl regard classif transport poi data first articl look use variou machin learn model classifi record airport stop train station second...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>700</td>\n",
       "      <td>3 metrics to make sure your app is delivering the best experience to its users. | by Abdo Shalaby | Towards Data Science</td>\n",
       "      <td>Abdo Shalaby</td>\n",
       "      <td>https://towardsdatascience.com/3-metrics-to-make-sure-your-app-is-delivering-the-best-experience-to-its-users-93adb248553e?source=collection_archive---------28-----------------------</td>\n",
       "      <td>toward data scienc aug save last post introduc simpl step framework organis think track app post want share three metric found help product team monitor app health statu let discus metric usag met...</td>\n",
       "      <td>2021-08-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "526    526   \n",
       "354    354   \n",
       "168    168   \n",
       "135    135   \n",
       "937    937   \n",
       "...    ...   \n",
       "1074  1074   \n",
       "1315  1315   \n",
       "383    383   \n",
       "1273  1273   \n",
       "700    700   \n",
       "\n",
       "                                                                                                                                            title  \\\n",
       "526                     Feature Selection, Binning, ANOVA, polynomial features, log transform, automatic feature selection | Towards Data Science   \n",
       "354                                      Top Python libraries for Image Augmentation in Computer Vision | by Kenneth Leung | Towards Data Science   \n",
       "168                                        Image Classification of PCBs and its Web Application (Flask) | by Utkarsh Ankit | Towards Data Science   \n",
       "135                              Variational Bayesian inference with normalizing flows: a simple example | by Fraser Lewis | Towards Data Science   \n",
       "937                                     What does Vertex AI do?. Your one-stop shop for machine learning | by Omer Mahmood | Towards Data Science   \n",
       "...                                                                                                                                           ...   \n",
       "1074  Machine Learning with only SQL — Using BigQuery to Identify the Target Audience for Shared Bike | by Wan Chung Huang | Towards Data Science   \n",
       "1315                                                      Build a Dash app with Python in 7 minutes | by Natassha Selvaraj | Towards Data Science   \n",
       "383                                       Styleformer: Convert Casual Text to Formal Text and Vice Versa | by Eric Fillion | Towards Data Science   \n",
       "1273                                          Ensemble Classification: A Brief Overview With Examples | by Pranav Thaenraj | Towards Data Science   \n",
       "700                      3 metrics to make sure your app is delivering the best experience to its users. | by Abdo Shalaby | Towards Data Science   \n",
       "\n",
       "                 author  \\\n",
       "526       Ibrahim Kovan   \n",
       "354       Kenneth Leung   \n",
       "168       Utkarsh Ankit   \n",
       "135        Fraser Lewis   \n",
       "937        Omer Mahmood   \n",
       "...                 ...   \n",
       "1074    Wan Chung Huang   \n",
       "1315  Natassha Selvaraj   \n",
       "383        Eric Fillion   \n",
       "1273    Pranav Thaenraj   \n",
       "700        Abdo Shalaby   \n",
       "\n",
       "                                                                                                                                                                                                       link  \\\n",
       "526               https://towardsdatascience.com/an-overview-of-data-preprocessing-features-enrichment-automatic-feature-selection-60b0c12d75ad?source=collection_archive---------14-----------------------   \n",
       "354                                   https://towardsdatascience.com/top-python-libraries-for-image-augmentation-in-computer-vision-2566bed0533e?source=collection_archive---------2-----------------------   \n",
       "168                                       https://towardsdatascience.com/image-classification-of-pcbs-and-its-web-application-flask-c2b26039924a?source=collection_archive---------8-----------------------   \n",
       "135                           https://towardsdatascience.com/variational-bayesian-inference-with-normalizing-flows-a-simple-example-1db109d91062?source=collection_archive---------7-----------------------   \n",
       "937                                                                           https://towardsdatascience.com/what-does-vertex-ai-do-d30014024f55?source=collection_archive---------9-----------------------   \n",
       "...                                                                                                                                                                                                     ...   \n",
       "1074  https://towardsdatascience.com/machine-learning-with-only-sql-using-bigquery-to-identify-the-target-audience-for-shared-bike-aa3a4041be3a?source=collection_archive---------18-----------------------   \n",
       "1315                                                       https://towardsdatascience.com/build-a-dash-app-with-python-in-7-minutes-72b6cca7d268?source=collection_archive---------3-----------------------   \n",
       "383                                   https://towardsdatascience.com/styleformer-convert-casual-text-to-formal-text-and-vice-versa-9cdc52abeaf5?source=collection_archive---------31-----------------------   \n",
       "1273                                         https://towardsdatascience.com/ensemble-classification-a-brief-overview-with-examples-3dac25613073?source=collection_archive---------25-----------------------   \n",
       "700                  https://towardsdatascience.com/3-metrics-to-make-sure-your-app-is-delivering-the-best-experience-to-its-users-93adb248553e?source=collection_archive---------28-----------------------   \n",
       "\n",
       "                                                                                                                                                                                                         text  \\\n",
       "526   toward data scienc aug save dataset render suitabl data train machin learn predict made algorithm yield success result look dataset seen featur import other impact output exampl better result obta...   \n",
       "354   toward data scienc aug save deep learn task like comput vision highli depend larg number imag train techniqu like transfer learn reduc amount data need dataset suffici qualiti quantiti varieti sti...   \n",
       "168   toward data scienc aug save hello blog creat imag classif model pcb print circuit board detect defect pcb classifi good bad creat deep learn model tri get best possibl result along proper visualis...   \n",
       "135   toward data scienc aug save variat infer methodolog watch larg complex bayesian model potenti becom approach least comput fit natur domain machin learn articl demonstr normal flow briefli introduc...   \n",
       "937   toward data scienc aug save post cover common task typic machin learn workflow vertex bring togeth tool need achiev one unifi user interfac today data scientist grappl challeng manual piec togeth ...   \n",
       "...                                                                                                                                                                                                       ...   \n",
       "1074  toward data scienc aug save mani reason use python machin learn matter case share machin learn sql understand key featur influenc durat bike share trip observ develop data warehous averag durat bi...   \n",
       "1315  toward data scienc aug save start work capston project month ago want creat interact dashboard machin learn model want dashboard form web applic display live updat new data enter system explor man...   \n",
       "383   toward data scienc aug save styleform brand new python librari allow chang style text use power transform model call tutori focus abil convert casual text formal text vice versa chang formal text ...   \n",
       "1273  toward data scienc aug save note articl final articl seri articl regard classif transport poi data first articl look use variou machin learn model classifi record airport stop train station second...   \n",
       "700   toward data scienc aug save last post introduc simpl step framework organis think track app post want share three metric found help product team monitor app health statu let discus metric usag met...   \n",
       "\n",
       "             day  \n",
       "526   2021-08-02  \n",
       "354   2021-08-02  \n",
       "168   2021-08-02  \n",
       "135   2021-08-02  \n",
       "937   2021-08-02  \n",
       "...          ...  \n",
       "1074  2021-08-02  \n",
       "1315  2021-08-02  \n",
       "383   2021-08-02  \n",
       "1273  2021-08-02  \n",
       "700   2021-08-02  \n",
       "\n",
       "[320 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "df_test#[[\"id\", \"title\"]]#.iloc[1507].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{526: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 354: ['augment', 'imag', 'star', 'librari', 'github'],\n",
       " 168: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 135: ['surrog', 'distribut', 'elbo', 'model', 'flow'],\n",
       " 937: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 1544: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 1253: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 237: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 478: ['perceptilab', 'model', 'layer', 'graphic', 'learn'],\n",
       " 650: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 514: ['augment', 'imag', 'star', 'librari', 'github'],\n",
       " 1103: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 968: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 1569: ['queri', 'field', 'elasticsearch', 'document', 'movi'],\n",
       " 584: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 65: ['queri', 'field', 'elasticsearch', 'document', 'movi'],\n",
       " 30: ['perceptilab', 'model', 'layer', 'graphic', 'learn'],\n",
       " 904: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 367: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 1134: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 1487: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 485: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 1548: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 374: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 29: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 1142: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 958: ['perceptilab', 'model', 'layer', 'graphic', 'learn'],\n",
       " 1466: ['dbt', 'issu', 'tabl', 'reserv', 'snowflak'],\n",
       " 332: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 712: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 70: ['stain', 'domain', 'imag', 'color', 'normal'],\n",
       " 289: ['queri', 'field', 'elasticsearch', 'document', 'movi'],\n",
       " 987: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 1526: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 416: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 1087: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 324: ['yolox', 'roboflow', 'command', 'model', 'train'],\n",
       " 617: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 380: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 173: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 634: ['dbt', 'issu', 'tabl', 'reserv', 'snowflak'],\n",
       " 701: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 1521: ['translat', 'model', 'languag', 'helsinki', 'text'],\n",
       " 23: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 1523: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 32: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 566: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 99: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 518: ['stain', 'domain', 'imag', 'color', 'normal'],\n",
       " 1187: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 1352: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 1175: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 807: ['surrog', 'distribut', 'elbo', 'model', 'flow'],\n",
       " 394: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 1395: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 1452: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 352: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 570: ['dbt', 'issu', 'tabl', 'reserv', 'snowflak'],\n",
       " 415: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 611: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 300: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 69: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 1493: ['busi', 'strategi', 'question', 'price', 'bank'],\n",
       " 1036: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 782: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 59: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 636: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 1018: ['dbt', 'issu', 'tabl', 'reserv', 'snowflak'],\n",
       " 759: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 677: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 1292: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 527: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 1553: ['translat', 'model', 'languag', 'helsinki', 'text'],\n",
       " 817: ['translat', 'model', 'languag', 'helsinki', 'text'],\n",
       " 244: ['season', 'seri', 'fourier', 'function', 'frequenc'],\n",
       " 1043: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 1410: ['augment', 'imag', 'star', 'librari', 'github'],\n",
       " 1073: ['translat', 'model', 'languag', 'helsinki', 'text'],\n",
       " 1462: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 1429: ['busi', 'strategi', 'question', 'price', 'bank'],\n",
       " 1205: ['busi', 'strategi', 'question', 'price', 'bank'],\n",
       " 1107: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 1398: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 535: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 1567: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 1303: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 298: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 764: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 344: ['team', 'skill', 'busi', 'talent', 'upskil'],\n",
       " 530: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 1084: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 534: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 1285: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 471: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 610: ['augment', 'imag', 'star', 'librari', 'github'],\n",
       " 185: ['ensembl', 'poi', 'model', 'safegraph', 'learner'],\n",
       " 707: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 49: ['translat', 'model', 'languag', 'helsinki', 'text'],\n",
       " 1114: ['dbt', 'issu', 'tabl', 'reserv', 'snowflak'],\n",
       " 818: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 124: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 903: ['surrog', 'distribut', 'elbo', 'model', 'flow'],\n",
       " 339: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 834: ['augment', 'imag', 'star', 'librari', 'github'],\n",
       " 907: ['write', 'work', 'book', 'day', 'morn'],\n",
       " 309: ['busi', 'strategi', 'question', 'price', 'bank'],\n",
       " 366: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 361: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 601: ['ensembl', 'poi', 'model', 'safegraph', 'learner'],\n",
       " 1177: ['ensembl', 'poi', 'model', 'safegraph', 'learner'],\n",
       " 1405: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 405: ['busi', 'strategi', 'question', 'price', 'bank'],\n",
       " 1116: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 1339: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 561: ['translat', 'model', 'languag', 'helsinki', 'text'],\n",
       " 1138: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 247: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 109: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 220: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 203: ['write', 'work', 'book', 'day', 'morn'],\n",
       " 847: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 1225: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 275: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 462: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 1364: ['season', 'seri', 'fourier', 'function', 'frequenc'],\n",
       " 350: ['perceptilab', 'model', 'layer', 'graphic', 'learn'],\n",
       " 270: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 598: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 51: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 76: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 551: ['surrog', 'distribut', 'elbo', 'model', 'flow'],\n",
       " 1489: ['translat', 'model', 'languag', 'helsinki', 'text'],\n",
       " 1460: ['season', 'seri', 'fourier', 'function', 'frequenc'],\n",
       " 1113: ['ensembl', 'poi', 'model', 'safegraph', 'learner'],\n",
       " 614: ['stain', 'domain', 'imag', 'color', 'normal'],\n",
       " 162: ['augment', 'imag', 'star', 'librari', 'github'],\n",
       " 123: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 1409: ['queri', 'field', 'elasticsearch', 'document', 'movi'],\n",
       " 438: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 855: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 1244: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 251: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 271: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 184: ['team', 'skill', 'busi', 'talent', 'upskil'],\n",
       " 67: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 483: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 141: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 1389: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 932: ['yolox', 'roboflow', 'command', 'model', 'train'],\n",
       " 528: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 464: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 715: ['write', 'work', 'book', 'day', 'morn'],\n",
       " 1001: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 306: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 765: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 1403: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 1421: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 591: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 1458: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 297: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 1379: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 938: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 1197: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 730: ['dbt', 'issu', 'tabl', 'reserv', 'snowflak'],\n",
       " 1145: ['ensembl', 'poi', 'model', 'safegraph', 'learner'],\n",
       " 1380: ['yolox', 'roboflow', 'command', 'model', 'train'],\n",
       " 888: ['team', 'skill', 'busi', 'talent', 'upskil'],\n",
       " 1538: ['augment', 'imag', 'star', 'librari', 'github'],\n",
       " 560: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 925: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 382: ['perceptilab', 'model', 'layer', 'graphic', 'learn'],\n",
       " 1065: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 239: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 1298: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 451: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 316: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 1163: ['write', 'work', 'book', 'day', 'morn'],\n",
       " 331: ['write', 'work', 'book', 'day', 'morn'],\n",
       " 978: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 554: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 590: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 303: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 1185: ['queri', 'field', 'elasticsearch', 'document', 'movi'],\n",
       " 170: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 1088: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 261: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 575: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 44: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 468: ['season', 'seri', 'fourier', 'function', 'frequenc'],\n",
       " 259: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 544: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 941: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 353: ['queri', 'field', 'elasticsearch', 'document', 'movi'],\n",
       " 398: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 842: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 752: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 529: ['translat', 'model', 'languag', 'helsinki', 'text'],\n",
       " 1037: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 115: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 736: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 836: ['yolox', 'roboflow', 'command', 'model', 'train'],\n",
       " 867: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 175: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 342: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 1317: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 413: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 1211: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 78: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 1144: ['team', 'skill', 'busi', 'talent', 'upskil'],\n",
       " 1474: ['augment', 'imag', 'star', 'librari', 'github'],\n",
       " 1054: ['perceptilab', 'model', 'layer', 'graphic', 'learn'],\n",
       " 1533: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 615: ['surrog', 'distribut', 'elbo', 'model', 'flow'],\n",
       " 585: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 198: ['stain', 'domain', 'imag', 'color', 'normal'],\n",
       " 15: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 786: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 1132: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 274: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 1368: ['team', 'skill', 'busi', 'talent', 'upskil'],\n",
       " 1031: ['surrog', 'distribut', 'elbo', 'model', 'flow'],\n",
       " 720: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 893: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 1322: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 672: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 1245: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 493: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 654: ['bin', 'featur', 'dataset', 'column', 'select'],\n",
       " 199: ['surrog', 'distribut', 'elbo', 'model', 'flow'],\n",
       " 607: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 1329: ['translat', 'model', 'languag', 'helsinki', 'text'],\n",
       " 906: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 1226: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 1591: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 351: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 1128: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 381: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 819: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 983: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 1176: ['team', 'skill', 'busi', 'talent', 'upskil'],\n",
       " 1004: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 1083: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 808: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 1085: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 240: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 432: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 56: ['team', 'skill', 'busi', 'talent', 'upskil'],\n",
       " 73: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 1157: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 886: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 1029: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 724: ['season', 'seri', 'fourier', 'function', 'frequenc'],\n",
       " 1399: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 43: ['write', 'work', 'book', 'day', 'morn'],\n",
       " 100: ['yolox', 'roboflow', 'command', 'model', 'train'],\n",
       " 1388: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 1155: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 101: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 631: ['interview', 'gbm', 'round', 'topic', 'explain'],\n",
       " 107: ['write', 'work', 'book', 'day', 'morn'],\n",
       " 1080: ['team', 'skill', 'busi', 'talent', 'upskil'],\n",
       " 1260: ['notebook', 'file', 'code', 'line', 'script'],\n",
       " 682: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 1232: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 1093: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 582: ['stain', 'domain', 'imag', 'color', 'normal'],\n",
       " 787: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 1214: ['perceptilab', 'model', 'layer', 'graphic', 'learn'],\n",
       " 128: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 425: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 1581: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 1318: ['stain', 'domain', 'imag', 'color', 'normal'],\n",
       " 1589: ['busi', 'strategi', 'question', 'price', 'bank'],\n",
       " 1595: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 500: ['season', 'seri', 'fourier', 'function', 'frequenc'],\n",
       " 1560: ['team', 'skill', 'busi', 'talent', 'upskil'],\n",
       " 522: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 1436: ['app', 'event', 'flow', 'metric', 'user'],\n",
       " 371: ['model', 'serv', 'system', 'platform', 'azur'],\n",
       " 864: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 543: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 909: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 481: ['queri', 'field', 'elasticsearch', 'document', 'movi'],\n",
       " 1517: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 411: ['data', 'spatial', 'raster', 'rainfal', 'coordin'],\n",
       " 1181: ['machin', 'model', 'learn', 'busi', 'drift'],\n",
       " 997: ['price', 'elast', 'product', 'competitor', 'cross'],\n",
       " 231: ['surrog', 'distribut', 'elbo', 'model', 'flow'],\n",
       " 1366: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 429: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 163: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 1296: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 266: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 1005: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 873: ['vertex', 'model', 'imag', 'train', 'automl'],\n",
       " 692: ['season', 'seri', 'fourier', 'function', 'frequenc'],\n",
       " 1450: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 1263: ['summar', 'ctrlsum', 'text', 'summari', 'queri'],\n",
       " 192: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 548: ['yolox', 'roboflow', 'command', 'model', 'train'],\n",
       " 63: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 772: ['yolox', 'roboflow', 'command', 'model', 'train'],\n",
       " 966: ['stain', 'domain', 'imag', 'color', 'normal'],\n",
       " 1023: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 651: ['write', 'work', 'book', 'day', 'morn'],\n",
       " 1312: ['author', 'collag', 'paper', 'layer', 'network'],\n",
       " 858: ['dbt', 'issu', 'tabl', 'reserv', 'snowflak'],\n",
       " 816: ['earthquak', 'kepler', 'data', 'wave', 'map'],\n",
       " 629: ['busi', 'strategi', 'question', 'price', 'bank'],\n",
       " 218: ['dbt', 'issu', 'tabl', 'reserv', 'snowflak'],\n",
       " 1578: ['label', 'snorkel', 'model', 'weak', 'gener'],\n",
       " 286: ['perceptilab', 'model', 'layer', 'graphic', 'learn'],\n",
       " 1288: ['imag', 'train', 'pcb', 'model', 'folder'],\n",
       " 1174: ['optim', 'wolfram', 'gurobi', 'problem', 'solver'],\n",
       " 589: ['mito', 'data', 'column', 'new', 'pivot'],\n",
       " 1074: ['data', 'durat', 'bigqueri', 'bikeshar', 'assumpt'],\n",
       " 1315: ['dash', 'chart', 'restaur', 'app', 'layout'],\n",
       " 383: ['styleform', 'text', 'textblob', 'formal', 'token'],\n",
       " 1273: ['ensembl', 'poi', 'model', 'safegraph', 'learner'],\n",
       " 700: ['app', 'event', 'flow', 'metric', 'user']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_res#[1417]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jane')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "47b72b2b651b2f1821dd9d098f668161e50304b55c5d2aa9a2af636fdcfbd008"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
